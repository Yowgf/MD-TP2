{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook performs task 2.3 of CRISP-DM on our King County housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to plot\n",
    "#   1. 2 or 3 distributions of some simple relationships,\n",
    "#      just to give a taste of the data.\n",
    "#   2. Factor SVD to find out how the data is distributed\n",
    "#   3. Measure to what extent the data is randomly clusterable\n",
    "# Won't use seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.utils import *\n",
    "df = loadPreprocessed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Plot simple distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont have time to learn seaborn\n",
    "# Will just use matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"]     = (12, 8)\n",
    "plt.rcParams[\"axes.labelsize\"]     = 11\n",
    "plt.rcParams[\"axes.labelweight\"]   = \"bold\"\n",
    "plt.rcParams[\"scatter.edgecolors\"] = \"white\"\n",
    "plt.rcParams[\"lines.linewidth\"]    = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot relation sqft_living X price\n",
    "\n",
    "plt.xlabel(\"sqft_living\")\n",
    "plt.ylabel(\"price - million US$\")\n",
    "plt.scatter(df.sqft_living.values, df.price.values)\n",
    "curLocs, _ = plt.yticks()\n",
    "plt.yticks(curLocs[1:-1], [str(int(price)) for price in curLocs[1:-1] / 1e6], color='k')\n",
    "\n",
    "plt.savefig(\"doc/history/cdm-23/sqft_living-price.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"sqft_living15\")\n",
    "plt.ylabel(\"price - million US$\")\n",
    "plt.scatter(df.sqft_living15.values, df.price.values)\n",
    "curLocs, _ = plt.yticks()\n",
    "plt.yticks(curLocs[1:-1], [str(int(price)) for price in curLocs[1:-1] / 1e6])\n",
    "\n",
    "plt.savefig(\"doc/history/cdm-23/sqft_living15-price.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical attribute waterfront and its effect on \"price\"\n",
    "# Measures shown with one standard deviation error\n",
    "\n",
    "withoutWaterDf = df[df.waterfront == 0].price\n",
    "withoutWaterMean = withoutWaterDf.mean()\n",
    "withoutWaterStd = withoutWaterDf.std()\n",
    "withoutWaterRange = np.array([withoutWaterMean - withoutWaterStd, withoutWaterMean + withoutWaterStd], dtype=int)\n",
    "\n",
    "withWaterDf = df[df.waterfront == 1].price\n",
    "withWaterMean = withWaterDf.mean()\n",
    "withWaterStd = withWaterDf.std()\n",
    "withWaterRange = np.array([withWaterMean - withWaterStd, withWaterMean + withWaterStd], dtype=int)\n",
    "\n",
    "print(f\"Price without waterfront: {withoutWaterRange}\")\n",
    "print(f\"Price with waterfront: {withWaterRange}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotScatterMatrix(df):\n",
    "    hist_kwds = {\"hist_kwds\": {\"edgecolor\": \"k\", \"bins\": 30}}\n",
    "    scatter_kwds = {\"linewidth\": 0.2, \"edgecolor\": \"white\"}\n",
    "    scatter_matrix(df.iloc[::, :4], figsize=(13, 8), **hist_kwds, **scatter_kwds)\n",
    "    plt.savefig(\"doc/history/cdm-23/scatter-matrix.png\")\n",
    "    plt.show()\n",
    "\n",
    "plotScatterMatrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Factor SVD to find out how the data is distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe this reinforces the reasoning on choosing Min-Max technique\n",
    "\n",
    "sigmas = np.linalg.svd(df, compute_uv=False)\n",
    "\n",
    "def chooseBiggestSigmas(sigmas):\n",
    "    chosenSigmas = list()\n",
    "    head = 0\n",
    "    while sum(chosenSigmas) / sigmas.sum() < 0.95:\n",
    "        chosenSigmas.append(sigmas[head])\n",
    "        head += 1\n",
    "\n",
    "    return np.array(chosenSigmas)\n",
    "\n",
    "chosenSigmas = chooseBiggestSigmas(sigmas)\n",
    "print(f\"Chosen sigmas: {chosenSigmas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test clustering null hypothesis (Gap statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this worth doing?\n",
    "# - What information is it going to give me?\n",
    "# - How does that information help me achieve my data mining objective?\n",
    "# What are the steps required?\n",
    "# - Finish preprocessing the data (CHECK)\n",
    "# - Find out how to build the gap statistic\n",
    "#\n",
    "# Ok. I think I know how to build the gap statistic\n",
    "#\n",
    "# 1. I fix the number of samples \"t\"\n",
    "#\n",
    "# 2. I have to generate samples of the dataset following the\n",
    "#    Monte Carlo sampling methodology.\n",
    "#\n",
    "# 3. I then run K-means on each sample like 100 times, to find\n",
    "#    out the best clustering for that sample.\n",
    "#\n",
    "# 4. Build the W similarity matrix for each cluster (1..K).\n",
    "#    Calculate the W_in coefficient, which is just the sum of\n",
    "#    all the values of that matrix. (should end up with 3 \n",
    "#    nested loops for this part)\n",
    "#\n",
    "# 5. Get the mean \"mu\" and standard deviation of the W_in's\n",
    "#\n",
    "# 6. The gap statistic for K is then given as\n",
    "#    gap(K) = mu - log(W_in(original dataset))\n",
    "#\n",
    "# It measures the difference between the null hypothesis and\n",
    "#   our clustering on the original dataset, for K clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.preprocessing import *\n",
    "# Drop some columns for better clustering\n",
    "# df = df[[\"price\", \"sqft_living\", \"bathrooms\", \"bedrooms\", \"floors\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = np.linalg.svd(df, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, we can reduce the dimensionality to 2 dimensions\n",
    "#   (maybe it is not that good of an idea...)\n",
    "\n",
    "dim = len(chosenSigmas)\n",
    "aproxDf = u[::, :dim] @ np.diag(s[:dim])\n",
    "reDf = pd.DataFrame(aproxDf, columns=[\"pc\" + str(i) for i in range(dim)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data first\n",
    "normalize(reDf)\n",
    "\n",
    "# Try something to test my algorithm: separate the data\n",
    "# Into two clusters. The gap statistic should go positive\n",
    "reDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reduced dimension data\n",
    "plt.scatter(reDf.values[::, 0], reDf.values[::, 1], edgecolor=\"white\", linewidth=0.5)\n",
    "plt.xlabel(\"pc1\", size=11, weight=\"bold\")\n",
    "plt.ylabel(\"pc2\", size=11, weight=\"bold\")\n",
    "plt.savefig(\"doc/history/cdm-23/data2d.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. I fix the of samples \"t\"\n",
    "t = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. I have to generate samples of the dataset following the\n",
    "#    Monte Carlo sampling methodology.\n",
    "def generateSample(df, sampleSize):\n",
    "    minMaxs = (df.min(), df.max())\n",
    "    randSamp = np.zeros((sampSize, df.shape[1]))\n",
    "    for i in range(randSamp.shape[0]):\n",
    "        for j in range(randSamp.shape[1]):\n",
    "            randSamp[i, j] = np.random.rand() * (minMaxs[1][j] - minMaxs[0][j]) + minMaxs[0][j]\n",
    "\n",
    "    return randSamp\n",
    "\n",
    "# 3. I then run K-means on each sample like 100 times, to find\n",
    "#    out the best clustering for that sample.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def vecEucDist(vec1, vec2):\n",
    "    return np.sqrt(((vec1 - vec2) ** 2).sum(axis=1))\n",
    "\n",
    "def runKmeans(dataMatrix, K):    \n",
    "    numberOfRuns = 10\n",
    "    minError = np.inf\n",
    "    minModel = None\n",
    "    for run in range(numberOfRuns):\n",
    "        model = KMeans(n_clusters=K, random_state=None)\n",
    "        model.fit(dataMatrix)\n",
    "        \n",
    "        # Need to be able to measure clustering error\n",
    "        error = model.inertia_ # sqrError(dataMatrix, model)\n",
    "        \n",
    "        if error < minError:\n",
    "            minError = error\n",
    "            minModel = model\n",
    "    \n",
    "    return minModel\n",
    "    \n",
    "# 4. Build the W similarity matrix for each cluster (1..K).\n",
    "#    Calculate the W_in coefficient, which is just the sum of\n",
    "#    all the values of that matrix. (should end up with like 3 \n",
    "#    nested loops for this part)\n",
    "def mapClusterDict(dataMatrix, labels):\n",
    "    clusters = dict()\n",
    "    for cluster in np.unique(labels):\n",
    "        clusters[cluster] = list()\n",
    "        \n",
    "    for i in range(labels.size):\n",
    "        clusters[labels[i]].append(dataMatrix[i])\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def buildW(cPoints):\n",
    "    cSize = cPoints.shape[0]         # Cluster size\n",
    "    W = np.zeros((cSize, cSize))     # W has size cSize X cSize\n",
    "    for i in range(cSize):           # Go through all the points\n",
    "        W[i, i:] = vecEucDist(cPoints[i], cPoints[i:])\n",
    "    \n",
    "    return W\n",
    "\n",
    "def buildW_in(clusters, k):\n",
    "    cPoints = np.array(clusters[k]) # Cluster points\n",
    "    W_in = buildW(cPoints)\n",
    "    \n",
    "    return W_in\n",
    "    \n",
    "def calcWeightIn(dataMatrix, KMeansModel):\n",
    "    centroids = KMeansModel.cluster_centers_\n",
    "    labels = KMeansModel.labels_\n",
    "    \n",
    "    if centroids.size == 0 or labels.size == 0:\n",
    "        raise AttributeError(\"Invalid empty model\")\n",
    "    \n",
    "    # Build association cluster -> its points\n",
    "    clusters = mapClusterDict(dataMatrix, labels)\n",
    "    \n",
    "    # Build W_in for each cluster\n",
    "    numClusters = len(clusters.keys())\n",
    "    Ws = np.zeros(numClusters)\n",
    "    for k in range(numClusters):\n",
    "        W_in = buildW_in(clusters, k)\n",
    "        \n",
    "        # W_in only has upper right entries non-null, so\n",
    "        # we dont have to divide them by two\n",
    "        Ws[k] = W_in.sum()\n",
    "    \n",
    "    return Ws\n",
    "\n",
    "def calcAllSampleWeights(df, sampSize, K, t):\n",
    "    W_ins = np.zeros(t)\n",
    "    for sampIdx in range(t):\n",
    "        print(\"Iteration \", sampIdx)\n",
    "\n",
    "        randSamp = generateSample(df, sampSize)\n",
    "        bestKMeansRun = runKmeans(randSamp, K)\n",
    "\n",
    "        W_ins[sampIdx] = calcWeightIn(randSamp, bestKMeansRun).sum()\n",
    "    \n",
    "    return W_ins\n",
    "\n",
    "def computeGapStatistic(df, sampSize, K, t):\n",
    "    W_ins = calcAllSampleWeights(df, sampSize, K, t)\n",
    "\n",
    "    # 5. Get the mean \"mu\" and standard deviation of the W_in's\n",
    "    # use the logarithm\n",
    "    mu = np.log(W_ins).mean()\n",
    "    sigma = np.sqrt(((np.log(W_ins) - mu) ** 2).mean())\n",
    "\n",
    "    # 6. The gap statistic for K is then given as\n",
    "    #    gap(K) = mu - log(W_in(original dataset))\n",
    "    # Have to calculate the W_in of original dataset\n",
    "    bestKMeansRun = runKmeans(df.values, K)\n",
    "    expectW_in = calcWeightIn(df.values, bestKMeansRun).sum()\n",
    "    \n",
    "    return mu - np.log(expectW_in), sigma\n",
    "\n",
    "nIter = 9\n",
    "gaps = np.zeros(nIter)\n",
    "gapSigmas = np.zeros(nIter)\n",
    "sampSize = reDf.shape[0]\n",
    "for K in range(2, nIter + 2):\n",
    "    \n",
    "    gaps[K - 2], gapSigmas[K - 2] = computeGapStatistic(reDf, sampSize, K, t)\n",
    "    print(\"\\nCalculated gap statistic: \", gaps[K - 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.arange(2, len(gaps) + 2), gaps\n",
    "\n",
    "plt.figure(1, figsize=(12, 8))\n",
    "plt.errorbar(x, y, linewidth=1, xerr=None, yerr=2 * gapSigmas, ecolor=\"grey\", elinewidth=4)\n",
    "plt.xlabel(\"K\", weight=\"bold\", size=11)\n",
    "plt.ylabel(\"gap\", weight=\"bold\", size=11)\n",
    "plt.savefig(\"doc/history/cdm-23/gaps.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the grouping on the reduced dimension\n",
    "# dataframe, for optimal K\n",
    "\n",
    "optimalK = 3\n",
    "\n",
    "# Plot each cluster separately\n",
    "bestKMeansRun = runKmeans(reDf.values, optimalK)\n",
    "clusters = mapClusterDict(reDf.values, bestKMeansRun.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in clusters.keys():\n",
    "    cPoints = np.array(clusters[k])\n",
    "    plt.scatter(cPoints[::, 0], cPoints[::, 1], edgecolor=\"white\", linewidth=0.5)\n",
    "\n",
    "# Plot the centroids above\n",
    "centroids = np.array(bestKMeansRun.cluster_centers_)\n",
    "plt.scatter(centroids[::, 0], centroids[::, 1], color='k', label=\"centroids\")\n",
    "\n",
    "plt.xlabel(\"pc1\", size=11, weight=\"bold\")\n",
    "plt.ylabel(\"pc2\", size=11, weight=\"bold\")\n",
    "plt.legend(labels=[\"cluster 1\", \"cluster 2\", \"centroids\"])\n",
    "plt.savefig(\"doc/history/cdm-23/clustering2d.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit original dataset with optimalK clusters\n",
    "\n",
    "kmeansFit = runKmeans(df.values, optimalK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide and use an internal cluster\n",
    "#   measure, other than the gap statistic,\n",
    "#   to measure the quality of the clustering\n",
    "#   obtained.\n",
    "#\n",
    "# Options: C-Index, Dunn Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(9).reshape(3,3).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = buildW(df.values)\n",
    "W_flatten = np.array(0)\n",
    "for i in range(1000):\n",
    "    W_flatten = np.append(W_flatten, W[i, i:])\n",
    "\n",
    "W_flatten = np.array(W_flatten)\n",
    "del W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to compute the C-index?\n",
    "#\n",
    "# 1. Choose a number of distances \"N_in\" to be compared\n",
    "#\n",
    "# 2. Compute similarity matrix W\n",
    "#\n",
    "# 3. Compute weights W_in(already have method for that)\n",
    "#\n",
    "# 4. Return their mean\n",
    "def getKMin(dataMatrix, K):\n",
    "    mins = np.zeros(K)\n",
    "    for i in range(K):\n",
    "        minIndex = dataMatrix.argmin()\n",
    "        mins[i] = dataMatrix[]\n",
    "def cIndex(dataMatrix, KMeansModel):\n",
    "    clusters = mapClusterDict(dataMatrix, KMeansModel.labels_)\n",
    "    \n",
    "    # Build W_in for each cluster\n",
    "    numClusters = len(clusters.keys())\n",
    "    cIndexes = np.zeros(numClusters)\n",
    "    for k in range(numClusters):\n",
    "        W_in = buildW_in(clusters, k)\n",
    "        N_in = len(clusters[k])\n",
    "        \n",
    "        W_min = W_flat_sort[:N_in].sum()\n",
    "        W_max = W_flat_sort[-N_in:].sum()\n",
    "        \n",
    "        print(\"W_in\\n\", W_in)\n",
    "        print(\"N_in: \", N_in)\n",
    "        print(\"W_min\\n\", W_min)\n",
    "        print(\"W_max\\n\", W_max)\n",
    "        \n",
    "        cIndexes[k] = (W_in - W_min) / (W_max - W_min)\n",
    "        \n",
    "    return cIndexex.mean()\n",
    "    \n",
    "CI = cIndex(df.values, kmeansFit)\n",
    "CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(9)[-3:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
